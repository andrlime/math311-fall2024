\section{October 2, 2024}

\subsection{Law of Total Probability (properly)}
\begin{theorem}
    Take some probability space $(\Omega, \curlyf, \probability)$. If $B_1, B_2, \ldots \in \curlyf$ are mutually exclusive and exhaustive (see 4.3), for $A \in \curlyf$
    \begin{align}
        \probability(A) = \sum_{i} \probability(A \cap B_i)
    \end{align}
\end{theorem}
\begin{proof}
    Trivially,
    \begin{align}
        \probability(A) = \probability(A \cap \Omega)
    \end{align}
    But, $B_1 \cap \cdots = \Omega$. So this equals
    \begin{align}
        \probability(A) &= \probability\qty(A \cap (B_1 \cap \cdots))\\
        &= \probability\qty(\bigcup(A \cap B_i))
    \end{align}
    Since $B_i$ are disjoint, these are all disjoint, so this becomes
    \begin{align}
        \probability(A) = \sum_i \probability(A \cap B_i)
    \end{align}
\end{proof}

Recall conditional probability if $\probability(A) \ne 0$
\begin{align}
    \probability(B \mid A) = \dfrac{\probability(A \cap B)}{\probability(A)}
\end{align}

\begin{theorem}
    For $\probability(B_i) > 0$, Theorem 5.1 using conditional probabilities equals
    \begin{align}
        \probability(A) = \sum_i \probability(B_i) \probability(A \mid B_i)
    \end{align}
\end{theorem}

\subsection{Bayes' Formula}
\begin{proposition}
    For $B_1, B_2, \ldots$ mutually exclusive and exhaustive,
    \begin{align}
        \probability(B_k \mid A) = \dfrac{\probability(A \cap B_k)}{\probability(A)}
    \end{align}
    which can be rewritten into
    \begin{align}
        \boxed{\probability(B_k \mid A) = \dfrac{\probability(B_k) \probability(A \mid B_k)}{\sum_i \probability(B_i) \probability(A \mid B_i)}}
    \end{align}
\end{proposition}

\begin{example}
    Throw a die with outcome $i \in \{1, \ldots, 6\}$. Then, flip a coin $i$ times. Find the conditional probability that the dice landed on $3$ given at least one head was obtained.
    \begin{proof}[Solution]
        The professor drew a beautiful, branching tree. Each outcome of a dice corresponds to some probabilities in terms of number of heads. This can be used to easily compute the conditional probability, using Equation 5.9.
    \end{proof}
\end{example}

\subsection{Borel Sets}
Take $\Omega = \real$.
\begin{definition}
    The Borel $\sigma$-algebra on $\real$, $\mathcal{B}$, is the $\sigma$-algebra of subsets of $\real$ generated by\footnote{smallest $\sigma$-algebra containing} the closed intervals for all $[a,b]$ where $a \le b$.
\end{definition}
The elements of $\mathcal{B}$ are Borel sets.
\begin{proposition}
    Some facts
    \begin{align}
        (a,b) = \bigcup_{n=1}^\infty [a + 1/n, b - 1/n] \implies {(a,b) \in \mathcal{B}}
    \end{align}
    \begin{align}
        [a,\infty) = \bigcup_{n=1}^\infty [a, a + n] \implies {[a,\infty) \in \mathcal{B}}
    \end{align}
    \begin{align}
        [z,z] \in \mathcal{B} \, \forall z \in \integer \implies \integer \in \mathcal{B}
    \end{align}
\end{proposition}

\subsubsection{Example of a probability space involving $\mathcal{B}$}
Let $\Omega = \real$ and $\curlyf = \mathcal{B}$. Let $f(x)$ be a normalized and non-negative (Riemann) integrable function on $\real$. Then, define the probability measure
\begin{align}
    \probability(B) = \int_{B} f(x) \dd{x}
\end{align}
But how do we integrate over a Borel set? We use the unique probability measure $\probability$ on $\mathcal{B}$ and redefine
\begin{align}
    \probability([a,b]) = \int_a^b f(x) \dd{x}
\end{align}
\begin{proof}
    This is measure theory so the professor refused.
\end{proof}

\subsection{Random Variables}
A random variable is a function on the sample space $\Omega$ which we want to measure.
\begin{definition}
    Let $(\Omega, \curlyf, \probability)$ be a probability space. A random variable is a real-valued function
    \begin{align}
        R: \Omega \to \real
    \end{align}
    such that for $a,b\in\real$ with $a\le b$, the set $\{ \omega \in \Omega \mid a \le R(\omega) \le b \}$ is an element of $\curlyf$. This is equivalent to the inverse image of $R^{-1}([a,b])$.
\end{definition}
\begin{proposition}
    If $\curlyf = 2^\Omega$, then every function $R: \Omega \to \real$ is a random variable.
\end{proposition}
\begin{example}
    Flip a coin 6 times. $\Omega$ is all of the possible outcomes. The function
    \begin{align}
        R = \text{number of heads}
    \end{align}
    is a random variable. Then, the probability
    \begin{align}
        \probability(0 \le R \le 1) = \frac{1}{2^6} + \frac{6}{2^6}
    \end{align}
\end{example}
\begin{example}
    Roll two dice. $\Omega$ is all of the possible outcomes. So
    \begin{align}
        \Omega = \left\{ (i,j) \mid i, j \in \{ 1, \ldots, 6 \} \right\}
    \end{align}
    Define a random variable $R = i + j$. We can compute
    \begin{align}
        \probability(0 \le R \le 3) = 0 + 0 + 1/36 + 2/36 = 1/12
    \end{align}
\end{example}
Then, a less trivial example:
\begin{example}
    Take $\Omega = \real$ and $\curlyf = \mathcal{B}$ and probability measure on function $f(x)$. Define
    \begin{align}
        R: \real \to \real \qsp R(x) = x + 1
    \end{align}
    Then,
    \begin{align}
        R^{-1}([a,b]) = [a-1,b-1]
    \end{align}
    which is obviously an element of $\mathcal{B}$.
\end{example}
Most functions (and most continuous ones) are random variables.
